{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1349d539",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4be2d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Downloads\\CSE150B\\pa1-finding-paths-MichaelPena7\\anaconda\\envs\\cse158\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from loader import load_to_dict, load_user_likes, save_likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09182b",
   "metadata": {},
   "source": [
    "Variables and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "584d60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"review-Washington_10.json.gz\"\n",
    "\n",
    "# Data settings\n",
    "POS_THRESHOLD = 4\n",
    "MIN_INTERACTIONS_PER_USER = 2\n",
    "MAX_USERS = 150000\n",
    "MAX_ITEMS = 150000\n",
    "\n",
    "# hyperparams\n",
    "LATENT_DIM = 20\n",
    "LEARNING_RATE = 0.05\n",
    "REG_LAMBDA = 1e-5\n",
    "NSAMPLES_PER_BATCH = 20000\n",
    "N_TRAIN_STEPS = 250\n",
    "TOP_K = 30\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89538b",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38bac91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10192020,\n",
       "                  user_id              name           time  rating  \\\n",
       " 0  103093043835388050629     Casper Steele  1626907411534       2   \n",
       " 1  111014066796803341223         Judy Maes  1613028426989       5   \n",
       " 2  111724423355988809570  Daniel Hernandez  1536710665852       5   \n",
       " 3  115331094085411087904         Lumi Nosa  1484359806540       5   \n",
       " 4  106906383883851362879    James Conright  1500643741079       1   \n",
       " \n",
       "                                                 text  pics  resp  \\\n",
       " 0  Drivers say security has bathroom key. Securit...  None  None   \n",
       " 1  It's a CTRAN transit center with schedules to ...  None  None   \n",
       " 2  Neat little Transit Center with lots of schedu...  None  None   \n",
       " 3                              Very helpful.  Thanks  None  None   \n",
       " 4                             They don't sell tacos.  None  None   \n",
       " \n",
       "                                  gmap_id  \n",
       " 0  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  \n",
       " 1  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  \n",
       " 2  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  \n",
       " 3  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  \n",
       " 4  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = load_to_dict(DATA_FILE)\n",
    "\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "len(df), df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b03de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full positive set (after MIN_INTERACTIONS_PER_USER):\n",
      "rows: 8407046 unique users: 331442 unique items: 72412\n"
     ]
    }
   ],
   "source": [
    "df_pos_full = df[df[\"rating\"] >= POS_THRESHOLD].copy()\n",
    "\n",
    "user_counts = df_pos_full[\"user_id\"].value_counts()\n",
    "eligible_users = user_counts[user_counts >= MIN_INTERACTIONS_PER_USER].index\n",
    "\n",
    "df_pos_full = df_pos_full[df_pos_full[\"user_id\"].isin(eligible_users)].reset_index(drop=True)\n",
    "\n",
    "print(\"Full positive set (after MIN_INTERACTIONS_PER_USER):\")\n",
    "print(\n",
    "    \"rows:\", len(df_pos_full),\n",
    "    \"unique users:\", df_pos_full[\"user_id\"].nunique(),\n",
    "    \"unique items:\", df_pos_full[\"gmap_id\"].nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38786813",
   "metadata": {},
   "source": [
    "Building BPR Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe5bfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPR subset:\n",
      "num_users: 150000 num_items: 72396\n"
     ]
    }
   ],
   "source": [
    "df_pos = df_pos_full.copy()\n",
    "\n",
    "# limit users\n",
    "unique_users = df_pos[\"user_id\"].unique()\n",
    "if MAX_USERS is not None:\n",
    "    unique_users = unique_users[:MAX_USERS]\n",
    "df_pos = df_pos[df_pos[\"user_id\"].isin(unique_users)]\n",
    "\n",
    "# limit items\n",
    "unique_items = df_pos[\"gmap_id\"].unique()\n",
    "if MAX_ITEMS is not None:\n",
    "    unique_items = unique_items[:MAX_ITEMS]\n",
    "df_pos = df_pos[df_pos[\"gmap_id\"].isin(unique_items)].reset_index(drop=True)\n",
    "\n",
    "# Recompute the *subset* users/items after filtering\n",
    "subset_users = df_pos[\"user_id\"].unique()\n",
    "subset_items = df_pos[\"gmap_id\"].unique()\n",
    "\n",
    "user_id_to_idx = {u: idx for idx, u in enumerate(subset_users)}\n",
    "item_id_to_idx = {i: idx for idx, i in enumerate(subset_items)}\n",
    "\n",
    "df_pos[\"user_idx\"] = df_pos[\"user_id\"].map(user_id_to_idx)\n",
    "df_pos[\"item_idx\"] = df_pos[\"gmap_id\"].map(item_id_to_idx)\n",
    "\n",
    "num_users = len(user_id_to_idx)\n",
    "num_items = len(item_id_to_idx)\n",
    "\n",
    "idx_to_user_id = {idx: u for u, idx in user_id_to_idx.items()}\n",
    "idx_to_item_id = {idx: i for i, idx in item_id_to_idx.items()}\n",
    "\n",
    "print(\"BPR subset:\")\n",
    "print(\"num_users:\", num_users, \"num_items:\", num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd3135",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "805e1c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4982429, 150000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Sort once\n",
    "if \"time\" in df_pos.columns:\n",
    "    df_pos = df_pos.sort_values([\"user_idx\", \"time\"])\n",
    "else:\n",
    "    df_pos = df_pos.sample(frac=1.0, random_state=RANDOM_SEED)\n",
    "\n",
    "user_counts = df_pos[\"user_idx\"].value_counts()\n",
    "\n",
    "# Users with at least 2 interactions\n",
    "multi_users = user_counts[user_counts >= 2].index\n",
    "\n",
    "# 3. For users with 2+ interactions: last one is test\n",
    "test_df = (\n",
    "    df_pos[df_pos[\"user_idx\"].isin(multi_users)]\n",
    "    .groupby(\"user_idx\")\n",
    "    .tail(1)\n",
    ")\n",
    "\n",
    "train_df = df_pos.drop(test_df.index)\n",
    "\n",
    "# 5. Clean up indices\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bccdcf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4982429, 72396)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_train = list(\n",
    "    zip(\n",
    "        train_df[\"user_idx\"].astype(int).tolist(),\n",
    "        train_df[\"item_idx\"].astype(int).tolist(),\n",
    "        train_df[\"rating\"].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "items_per_user_train = defaultdict(set)\n",
    "for u, i, r in interactions_train:\n",
    "    items_per_user_train[u].add(i)\n",
    "\n",
    "all_items = list(range(num_items))\n",
    "\n",
    "len(interactions_train), len(all_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4295718",
   "metadata": {},
   "source": [
    "BPR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "774c4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRbatch(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super().__init__()\n",
    "        self.lamb = lamb\n",
    "\n",
    "        # Global item bias\n",
    "        self.betaI = self.add_weight(\n",
    "            name=\"betaI\",\n",
    "            shape=(num_items,),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # User latent factors\n",
    "        self.gammaU = self.add_weight(\n",
    "            name=\"gammaU\",\n",
    "            shape=(num_users, K),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Item latent factors\n",
    "        self.gammaI = self.add_weight(\n",
    "            name=\"gammaI\",\n",
    "            shape=(num_items, K),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def score(self, sampleU, sampleI):\n",
    "        # sampleU, sampleI are index tensors\n",
    "        u = tf.cast(sampleU, tf.int32)\n",
    "        i = tf.cast(sampleI, tf.int32)\n",
    "\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "\n",
    "        x_ui = beta_i + tf.reduce_sum(gamma_u * gamma_i, axis=1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        # BPR loss: -log σ(x_ui - x_uj)\n",
    "        loss = -tf.reduce_mean(tf.math.log_sigmoid(x_ui - x_uj))\n",
    "        return loss\n",
    "\n",
    "    def reg(self):\n",
    "        return self.lamb * (\n",
    "            tf.nn.l2_loss(self.betaI)\n",
    "            + tf.nn.l2_loss(self.gammaU)\n",
    "            + tf.nn.l2_loss(self.gammaI)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350c846",
   "metadata": {},
   "source": [
    "Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "567d76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "modelBPR = BPRbatch(LATENT_DIM, REG_LAMBDA)\n",
    "\n",
    "def trainingStepBPR(model, interactions, items_per_user, items, Nsamples):\n",
    "    sampleU, sampleI, sampleJ = [], [], []\n",
    "\n",
    "    for _ in range(Nsamples):\n",
    "        u, i, r = random.choice(interactions)\n",
    "        j = random.choice(items)\n",
    "        while j in items_per_user[u]:\n",
    "            j = random.choice(items)\n",
    "\n",
    "        sampleU.append(u)\n",
    "        sampleI.append(i)\n",
    "        sampleJ.append(j)\n",
    "\n",
    "    # Convert lists → tensors\n",
    "    sampleU_tf = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "    sampleI_tf = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "    sampleJ_tf = tf.convert_to_tensor(sampleJ, dtype=tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(sampleU_tf, sampleI_tf, sampleJ_tf)\n",
    "        loss += model.reg()\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Pair gradients with variables, skipping any None grads just in case\n",
    "    grads_and_vars = [\n",
    "        (g, v) for g, v in zip(grads, model.trainable_variables) if g is not None\n",
    "    ]\n",
    "\n",
    "    if grads_and_vars:\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "    else:\n",
    "        print(\"Warning: no gradients to apply this step.\")\n",
    "\n",
    "    return float(loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5aa43ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One mini-batch objective: 0.6932\n"
     ]
    }
   ],
   "source": [
    "test_loss = trainingStepBPR(\n",
    "    modelBPR,\n",
    "    interactions_train,\n",
    "    items_per_user_train,\n",
    "    all_items,\n",
    "    Nsamples=10000  # mini testing\n",
    ")\n",
    "\n",
    "print(f\"One mini-batch objective: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0a0d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, objective = 0.6138\n",
      "Step 20, objective = 0.5707\n",
      "Step 30, objective = 0.5509\n",
      "Step 40, objective = 0.5434\n",
      "Step 50, objective = 0.5327\n",
      "Step 60, objective = 0.5361\n",
      "Step 70, objective = 0.5325\n",
      "Step 80, objective = 0.5304\n",
      "Step 90, objective = 0.5305\n",
      "Step 100, objective = 0.5296\n",
      "Step 110, objective = 0.5344\n",
      "Step 120, objective = 0.5361\n",
      "Step 130, objective = 0.5343\n",
      "Step 140, objective = 0.5354\n",
      "Step 150, objective = 0.5396\n",
      "Step 160, objective = 0.5404\n",
      "Step 170, objective = 0.5430\n",
      "Step 180, objective = 0.5449\n",
      "Step 190, objective = 0.5438\n",
      "Step 200, objective = 0.5450\n",
      "Step 210, objective = 0.5511\n",
      "Step 220, objective = 0.5494\n",
      "Step 230, objective = 0.5495\n",
      "Step 240, objective = 0.5480\n",
      "Step 250, objective = 0.5448\n"
     ]
    }
   ],
   "source": [
    "for step in range(N_TRAIN_STEPS):\n",
    "    obj = trainingStepBPR(\n",
    "        modelBPR,\n",
    "        interactions_train,\n",
    "        items_per_user_train,\n",
    "        all_items,\n",
    "        NSAMPLES_PER_BATCH,\n",
    "    )\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step {step + 1}, objective = {obj:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2ca23",
   "metadata": {},
   "source": [
    "Grabbing Oregon Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18ffb2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oregon users with revealed likes: 204512\n",
      "Oregon users with hidden likes: 204512\n"
     ]
    }
   ],
   "source": [
    "users_revealed_likes = load_user_likes(\"users_revealed_likes.json\")\n",
    "users_hidden_likes = load_user_likes(\"users_hidden_likes.json\")\n",
    "\n",
    "print(\"Oregon users with revealed likes:\", len(users_revealed_likes))\n",
    "print(\"Oregon users with hidden likes:\", len(users_hidden_likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bpr_recommendations_for_oregon(model, k):\n",
    "    \"\"\"\n",
    "    Use a model trained on Washington to recommend items for Oregon users.\n",
    "    Returns: dict user_id -> list of gmap_ids (top-k recommendations)\n",
    "    \"\"\"\n",
    "    recs = {}\n",
    "\n",
    "    # All item indices that the model knows about (Washington items)\n",
    "    all_item_indices = np.arange(num_items, dtype=np.int32)\n",
    "\n",
    "    for user_id, hidden_items in users_hidden_likes.items():\n",
    "        # If this Oregon user never appeared in Washington training, we can't get an embedding\n",
    "        if user_id not in user_id_to_idx:\n",
    "            continue\n",
    "\n",
    "        u_idx = user_id_to_idx[user_id]\n",
    "\n",
    "        # Start with all known items\n",
    "        candidate_item_indices = all_item_indices.copy()\n",
    "\n",
    "        # Remove items that are \"revealed likes\" in Oregon (like baseline)\n",
    "        revealed_items = users_revealed_likes.get(user_id, [])\n",
    "        revealed_item_indices = [\n",
    "            item_id_to_idx[g] for g in revealed_items if g in item_id_to_idx\n",
    "        ]\n",
    "\n",
    "        if revealed_item_indices:\n",
    "            mask = ~np.isin(candidate_item_indices, revealed_item_indices)\n",
    "            candidate_item_indices = candidate_item_indices[mask]\n",
    "\n",
    "        if len(candidate_item_indices) == 0:\n",
    "            recs[user_id] = []\n",
    "            continue\n",
    "\n",
    "        # Score all candidate items for this user\n",
    "        u_list = np.full(len(candidate_item_indices), u_idx, dtype=np.int32)\n",
    "        scores = model.score(u_list, candidate_item_indices).numpy()\n",
    "\n",
    "        # Take top-k items by score\n",
    "        if len(candidate_item_indices) <= k:\n",
    "            top_item_indices = candidate_item_indices\n",
    "        else:\n",
    "            top_idx = np.argpartition(-scores, k - 1)[:k]\n",
    "            top_item_indices = candidate_item_indices[top_idx]\n",
    "\n",
    "        #sort those top-k by score descending for nicer order\n",
    "        u_list_top = np.full(len(top_item_indices), u_idx, dtype=np.int32)\n",
    "        top_scores = model.score(u_list_top, top_item_indices).numpy()\n",
    "        pairs = list(zip(top_item_indices, top_scores))\n",
    "        pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_item_indices_sorted = [i for i, s in pairs]\n",
    "\n",
    "        # Map from item indices back to original gmap_ids\n",
    "        top_item_ids = [idx_to_item_id[i] for i in top_item_indices_sorted]\n",
    "\n",
    "        recs[user_id] = top_item_ids\n",
    "\n",
    "    return recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878e65d",
   "metadata": {},
   "source": [
    "Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f31974",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items_per_user = defaultdict(list)\n",
    "for _, row in test_df.iterrows():\n",
    "    u = int(row[\"user_idx\"])\n",
    "    i = int(row[\"item_idx\"])\n",
    "    test_items_per_user[u].append(i)\n",
    "\n",
    "def evaluate_hit_rate_at_k(model, train_items_per_user, test_items_per_user, items, k):\n",
    "    users = list(test_items_per_user.keys())\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    # Convert items to a NumPy array once\n",
    "    all_items_array = np.array(items, dtype=np.int32)\n",
    "\n",
    "    for idx, u in enumerate(users):\n",
    "        train_items = train_items_per_user[u] \n",
    "\n",
    "        # Mask out training items\n",
    "        candidate_mask = ~np.isin(all_items_array, list(train_items))\n",
    "        candidate_items = all_items_array[candidate_mask]\n",
    "\n",
    "        if len(candidate_items) == 0:\n",
    "            continue\n",
    "\n",
    "        u_list = np.full(len(candidate_items), u, dtype=np.int32)\n",
    "        scores = model.score(u_list, candidate_items).numpy()\n",
    "\n",
    "        top_k_idx = np.argpartition(-scores, k - 1)[:k]\n",
    "        top_k_items = set(candidate_items[top_k_idx])\n",
    "\n",
    "        test_items = set(test_items_per_user[u])  \n",
    "        if top_k_items & test_items:\n",
    "            hits += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        # progress print every 10000 users so we know it's moving\n",
    "        if (idx + 1) % 10000 == 0:\n",
    "            print(f\"Evaluated {idx+ 1}/{len(users)} users...\")\n",
    "\n",
    "    if total == 0:\n",
    "        return None\n",
    "    return hits / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad19f496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, now building BPR recommendations for Oregon users...\n",
      "Number of Oregon users with BPR recs: 10965\n",
      "Saved to  eval/bpr_recommendation_per_user.json\n",
      "Saved BPR recs to bpr_recommendation_per_user.json\n",
      "Backed up original baseline recs to baseline_recommendation_per_user_baseline_backup.json\n",
      "Saved to  eval/baseline_recommendation_per_user.json\n",
      "Overwrote baseline_recommendation_per_user.json with BPR recs\n"
     ]
    }
   ],
   "source": [
    "print(\"Training finished, now building BPR recommendations for Oregon users...\")\n",
    "\n",
    "bpr_recs_original = build_bpr_recommendations_for_oregon(\n",
    "    modelBPR,\n",
    "    TOP_K,  \n",
    ")\n",
    "\n",
    "print(\"Number of Oregon users with BPR recs:\", len(bpr_recs_original))\n",
    "\n",
    "# Save in the same format as baseline_recommendation_per_user.json\n",
    "save_likes(\"bpr_recommendation_per_user.json\", bpr_recs_original)\n",
    "print(\"Saved BPR recs to bpr_recommendation_per_user.json\")\n",
    "\n",
    "\n",
    "shutil.copyfile(\n",
    "    \"eval/baseline_recommendation_per_user.json\",\n",
    "    \"eval/baseline_recommendation_per_user_baseline_backup.json\"\n",
    ")\n",
    "print(\"Backed up original baseline recs to baseline_recommendation_per_user_baseline_backup.json\")\n",
    "\n",
    "# Overwrite the file eval.ipynb uses with BPR recs\n",
    "save_likes(\"baseline_recommendation_per_user.json\", bpr_recs_original)\n",
    "print(\"Overwrote baseline_recommendation_per_user.json with BPR recs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8671ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 10000/150000 users...\n",
      "Evaluated 20000/150000 users...\n",
      "Evaluated 30000/150000 users...\n",
      "Evaluated 40000/150000 users...\n",
      "Evaluated 50000/150000 users...\n",
      "Evaluated 60000/150000 users...\n",
      "Evaluated 70000/150000 users...\n",
      "Evaluated 80000/150000 users...\n",
      "Evaluated 90000/150000 users...\n",
      "Evaluated 100000/150000 users...\n",
      "Evaluated 110000/150000 users...\n",
      "Evaluated 120000/150000 users...\n",
      "Evaluated 130000/150000 users...\n",
      "Evaluated 140000/150000 users...\n",
      "Evaluated 150000/150000 users...\n",
      "HitRate@30: 0.0091\n"
     ]
    }
   ],
   "source": [
    "print(TOP_K)\n",
    "hit_at_k = evaluate_hit_rate_at_k(\n",
    "    modelBPR,\n",
    "    items_per_user_train,\n",
    "    test_items_per_user,\n",
    "    all_items,\n",
    "    TOP_K,\n",
    ")\n",
    "\n",
    "print(f\"HitRate@{TOP_K}: {hit_at_k:.4f}\" if hit_at_k is not None else \"No users with test items to evaluate.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse158",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
