{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1349d539",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d4be2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from loader import load_to_dict, load_user_likes, save_likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89538b",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "584d60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"review-Oregon_10.json.gz\"\n",
    "\n",
    "POS_THRESHOLD = 4\n",
    "MIN_INTERACTIONS_PER_USER = 1\n",
    "MAX_USERS = 100000\n",
    "MAX_ITEMS = 100000\n",
    "\n",
    "LATENT_DIM = 64\n",
    "LEARNING_RATE = 0.01\n",
    "REG_LAMBDA = 1e-5\n",
    "NSAMPLES_PER_BATCH = 50000\n",
    "N_TRAIN_STEPS = 250\n",
    "TOP_K = 30\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d38bac91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6270332,\n",
       "                  user_id               name           time  rating  \\\n",
       " 0  116238557567455956213          Mike Alan  1607013385211       5   \n",
       " 1  116988773242398246268   Jennifer Zumwalt  1607571121183       5   \n",
       " 2  118003928746583471938  Courtney Saunders  1582762237754       5   \n",
       " 3  118003928746583471938  Courtney Saunders  1582762237754       5   \n",
       " 4  103097287486867336142       Becky proulx  1584064731306       5   \n",
       " \n",
       "                                                 text  pics  \\\n",
       " 0  Every staff member in this office is absolutel...  None   \n",
       " 1  They are amazing. Always so kinda and helpful....  None   \n",
       " 2  With three kids and myself, our family has bee...  None   \n",
       " 3  With three kids and myself, our family has bee...  None   \n",
       " 4  Three of our children have received their brac...  None   \n",
       " \n",
       "                                                 resp  \\\n",
       " 0                                               None   \n",
       " 1                                               None   \n",
       " 2  {'time': 1599964077016, 'text': 'It has been g...   \n",
       " 3  {'time': 1599964077016, 'text': 'It has been g...   \n",
       " 4  {'time': 1599961987996, 'text': 'I am glad you...   \n",
       " \n",
       "                                  gmap_id  \n",
       " 0  0x54bfff5952aad583:0xad7afdc825730614  \n",
       " 1  0x54bfff5952aad583:0xad7afdc825730614  \n",
       " 2  0x54bfff5952aad583:0xad7afdc825730614  \n",
       " 3  0x54bfff5952aad583:0xad7afdc825730614  \n",
       " 4  0x54bfff5952aad583:0xad7afdc825730614  )"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = load_to_dict(DATA_FILE)\n",
    "\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "len(df), df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e7b03de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5180611, 204512, 47119)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = df[df[\"rating\"] >= POS_THRESHOLD].copy()\n",
    "\n",
    "user_counts = df_pos[\"user_id\"].value_counts()\n",
    "eligible_users = user_counts[user_counts >= MIN_INTERACTIONS_PER_USER].index\n",
    "\n",
    "df_pos = df_pos[df_pos[\"user_id\"].isin(eligible_users)].reset_index(drop=True)\n",
    "\n",
    "len(df_pos), df_pos[\"user_id\"].nunique(), df_pos[\"gmap_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4fd12ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 47107)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users = df_pos[\"user_id\"].unique()\n",
    "if MAX_USERS is not None:\n",
    "    unique_users = unique_users[:MAX_USERS]\n",
    "\n",
    "df_pos = df_pos[df_pos[\"user_id\"].isin(unique_users)]\n",
    "\n",
    "unique_items = df_pos[\"gmap_id\"].unique()\n",
    "if MAX_ITEMS is not None:\n",
    "    unique_items = unique_items[:MAX_ITEMS]\n",
    "\n",
    "df_pos = df_pos[df_pos[\"gmap_id\"].isin(unique_items)].reset_index(drop=True)\n",
    "\n",
    "user_id_to_idx = {u: idx for idx, u in enumerate(unique_users)}\n",
    "item_id_to_idx = {i: idx for idx, i in enumerate(unique_items)}\n",
    "\n",
    "df_pos[\"user_idx\"] = df_pos[\"user_id\"].map(user_id_to_idx)\n",
    "df_pos[\"item_idx\"] = df_pos[\"gmap_id\"].map(item_id_to_idx)\n",
    "\n",
    "num_users = len(user_id_to_idx)\n",
    "num_items = len(item_id_to_idx)\n",
    "\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "805e1c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2671266, 717261)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle rows (if no timestamp, just randomize)\n",
    "if \"time\" in df_pos.columns:\n",
    "    df_pos = df_pos.sort_values([\"user_idx\", \"time\"])\n",
    "else:\n",
    "    df_pos = df_pos.sample(frac=1.0, random_state=RANDOM_SEED)\n",
    "\n",
    "train_rows = []\n",
    "test_rows = []\n",
    "\n",
    "# Split each user's interactions 80:20\n",
    "for user_id, group in df_pos.groupby(\"user_id\"):\n",
    "    idx_list = group.index.tolist()\n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    split_point = max(1, int(0.8 * len(idx_list)))  # ensure at least 1 row in train\n",
    "    train_rows.extend(idx_list[:split_point])\n",
    "    test_rows.extend(idx_list[split_point:])\n",
    "\n",
    "train_df = df_pos.loc[train_rows].reset_index(drop=True)\n",
    "test_df = df_pos.loc[test_rows].reset_index(drop=True)\n",
    "\n",
    "\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a4ebf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a translation dictionary so we know the user and location id we're referring to by their idx\n",
    "idx_to_user_id = {idx: u for u, idx in user_id_to_idx.items()}\n",
    "idx_to_item_id = {idx: i for i, idx in item_id_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bccdcf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2671266, 47107)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_train = list(\n",
    "    zip(\n",
    "        train_df[\"user_idx\"].astype(int).tolist(),\n",
    "        train_df[\"item_idx\"].astype(int).tolist(),\n",
    "        train_df[\"rating\"].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "items_per_user_train = defaultdict(set)\n",
    "for u, i, r in interactions_train:\n",
    "    items_per_user_train[u].add(i)\n",
    "\n",
    "all_items = list(range(num_items))\n",
    "\n",
    "len(interactions_train), len(all_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "774c4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRbatch(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super().__init__()\n",
    "        self.lamb = lamb\n",
    "\n",
    "        # Global item bias\n",
    "        self.betaI = self.add_weight(\n",
    "            name=\"betaI\",\n",
    "            shape=(num_items,),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # User latent factors\n",
    "        self.gammaU = self.add_weight(\n",
    "            name=\"gammaU\",\n",
    "            shape=(num_users, K),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Item latent factors\n",
    "        self.gammaI = self.add_weight(\n",
    "            name=\"gammaI\",\n",
    "            shape=(num_items, K),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def score(self, sampleU, sampleI):\n",
    "        # sampleU, sampleI are index tensors\n",
    "        u = tf.cast(sampleU, tf.int32)\n",
    "        i = tf.cast(sampleI, tf.int32)\n",
    "\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "\n",
    "        x_ui = beta_i + tf.reduce_sum(gamma_u * gamma_i, axis=1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        # BPR loss: -log σ(x_ui - x_uj)\n",
    "        loss = -tf.reduce_mean(tf.math.log_sigmoid(x_ui - x_uj))\n",
    "        return loss\n",
    "\n",
    "    def reg(self):\n",
    "        return self.lamb * (\n",
    "            tf.nn.l2_loss(self.betaI)\n",
    "            + tf.nn.l2_loss(self.gammaU)\n",
    "            + tf.nn.l2_loss(self.gammaI)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "567d76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "modelBPR = BPRbatch(LATENT_DIM, REG_LAMBDA)\n",
    "\n",
    "def trainingStepBPR(model, interactions, items_per_user, items, Nsamples):\n",
    "    sampleU, sampleI, sampleJ = [], [], []\n",
    "\n",
    "    for _ in range(Nsamples):\n",
    "        u, i, r = random.choice(interactions)\n",
    "        j = random.choice(items)\n",
    "        while j in items_per_user[u]:\n",
    "            j = random.choice(items)\n",
    "\n",
    "        sampleU.append(u)\n",
    "        sampleI.append(i)\n",
    "        sampleJ.append(j)\n",
    "\n",
    "    # Convert lists → tensors\n",
    "    sampleU_tf = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "    sampleI_tf = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "    sampleJ_tf = tf.convert_to_tensor(sampleJ, dtype=tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(sampleU_tf, sampleI_tf, sampleJ_tf)\n",
    "        loss += model.reg()\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Pair gradients with variables, skipping any None grads just in case\n",
    "    grads_and_vars = [\n",
    "        (g, v) for g, v in zip(grads, model.trainable_variables) if g is not None\n",
    "    ]\n",
    "\n",
    "    if grads_and_vars:\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "    else:\n",
    "        print(\"Warning: no gradients to apply this step.\")\n",
    "\n",
    "    return float(loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5aa43ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One mini-batch objective: 0.6932\n"
     ]
    }
   ],
   "source": [
    "test_loss = trainingStepBPR(\n",
    "    modelBPR,\n",
    "    interactions_train,\n",
    "    items_per_user_train,\n",
    "    all_items,\n",
    "    Nsamples=10000  # mini testing\n",
    ")\n",
    "\n",
    "print(f\"One mini-batch objective: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f0a0d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5, objective = 0.6807\n",
      "Step 10, objective = 0.6663\n",
      "Step 15, objective = 0.6528\n",
      "Step 20, objective = 0.6395\n",
      "Step 25, objective = 0.6289\n",
      "Step 30, objective = 0.6177\n",
      "Step 35, objective = 0.6077\n",
      "Step 40, objective = 0.6001\n",
      "Step 45, objective = 0.5909\n",
      "Step 50, objective = 0.5851\n",
      "Step 55, objective = 0.5767\n",
      "Step 60, objective = 0.5723\n",
      "Step 65, objective = 0.5665\n",
      "Step 70, objective = 0.5606\n",
      "Step 75, objective = 0.5578\n",
      "Step 80, objective = 0.5542\n",
      "Step 85, objective = 0.5511\n",
      "Step 90, objective = 0.5474\n",
      "Step 95, objective = 0.5442\n",
      "Step 100, objective = 0.5418\n",
      "Step 105, objective = 0.5401\n",
      "Step 110, objective = 0.5370\n",
      "Step 115, objective = 0.5355\n",
      "Step 120, objective = 0.5353\n",
      "Step 125, objective = 0.5331\n",
      "Step 130, objective = 0.5332\n",
      "Step 135, objective = 0.5317\n",
      "Step 140, objective = 0.5335\n",
      "Step 145, objective = 0.5310\n",
      "Step 150, objective = 0.5263\n",
      "Step 155, objective = 0.5296\n",
      "Step 160, objective = 0.5281\n",
      "Step 165, objective = 0.5263\n",
      "Step 170, objective = 0.5238\n",
      "Step 175, objective = 0.5251\n",
      "Step 180, objective = 0.5244\n",
      "Step 185, objective = 0.5207\n",
      "Step 190, objective = 0.5222\n",
      "Step 195, objective = 0.5214\n",
      "Step 200, objective = 0.5203\n",
      "Step 205, objective = 0.5190\n",
      "Step 210, objective = 0.5189\n",
      "Step 215, objective = 0.5188\n",
      "Step 220, objective = 0.5181\n",
      "Step 225, objective = 0.5168\n",
      "Step 230, objective = 0.5188\n",
      "Step 235, objective = 0.5153\n",
      "Step 240, objective = 0.5151\n",
      "Step 245, objective = 0.5162\n",
      "Step 250, objective = 0.5141\n"
     ]
    }
   ],
   "source": [
    "for step in range(N_TRAIN_STEPS):\n",
    "    obj = trainingStepBPR(\n",
    "        modelBPR,\n",
    "        interactions_train,\n",
    "        items_per_user_train,\n",
    "        all_items,\n",
    "        NSAMPLES_PER_BATCH,\n",
    "    )\n",
    "    if (step + 1) % 5 == 0:\n",
    "        print(f\"Step {step + 1}, objective = {obj:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878e65d",
   "metadata": {},
   "source": [
    "Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "18f31974",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items_per_user = defaultdict(list)\n",
    "for _, row in test_df.iterrows():\n",
    "    u = int(row[\"user_idx\"])\n",
    "    i = int(row[\"item_idx\"])\n",
    "    test_items_per_user[u].append(i)\n",
    "\n",
    "def evaluate_hit_rate_at_k(model, train_items_per_user, test_items_per_user, items, k):\n",
    "    users = list(test_items_per_user.keys())\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    # Convert items to a NumPy array once\n",
    "    all_items_array = np.array(items, dtype=np.int32)\n",
    "\n",
    "    for idx, u in enumerate(users):\n",
    "        train_items = train_items_per_user[u]  # this is already a set\n",
    "\n",
    "        # Mask out training items\n",
    "        candidate_mask = ~np.isin(all_items_array, list(train_items))\n",
    "        candidate_items = all_items_array[candidate_mask]\n",
    "\n",
    "        if len(candidate_items) == 0:\n",
    "            continue\n",
    "\n",
    "        u_list = np.full(len(candidate_items), u, dtype=np.int32)\n",
    "        scores = model.score(u_list, candidate_items).numpy()\n",
    "\n",
    "        top_k_idx = np.argpartition(-scores, k - 1)[:k]\n",
    "        top_k_items = set(candidate_items[top_k_idx])\n",
    "\n",
    "        test_items = set(test_items_per_user[u])  # usually size 1\n",
    "        if top_k_items & test_items:\n",
    "            hits += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        # Optional: progress print every 1000 users so you know it's moving\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"Evaluated {idx + 1}/{len(users)} users...\")\n",
    "\n",
    "    if total == 0:\n",
    "        return None\n",
    "    return hits / total\n",
    "\n",
    "def get_top_k_recommendations(model, train_items_per_user, test_items_per_user, items, k, idx_to_user_id, idx_to_item_id):\n",
    "    \"\"\"\n",
    "    Returns: dict of user_id -> list of top-k gmap_ids, excluding training items.\n",
    "    \"\"\"\n",
    "    recs = {}\n",
    "    users = list(test_items_per_user.keys())\n",
    "    all_items_array = np.array(items, dtype=np.int32)\n",
    "\n",
    "    for idx, u in enumerate(users):\n",
    "        user_id = idx_to_user_id[u]\n",
    "\n",
    "        train_items = train_items_per_user[u]\n",
    "\n",
    "        # Candidate items = all items not already interacted with\n",
    "        candidate_mask = ~np.isin(all_items_array, list(train_items))\n",
    "        candidate_items = all_items_array[candidate_mask]\n",
    "\n",
    "        if len(candidate_items) == 0:\n",
    "            recs[user_id] = []\n",
    "            continue\n",
    "\n",
    "        # Score all candidate items\n",
    "        u_list = np.full(len(candidate_items), u, dtype=np.int32)\n",
    "        scores = model.score(u_list, candidate_items).numpy()\n",
    "\n",
    "        # Sort scores and get top-k\n",
    "        top_idx = np.argsort(-scores)[:k]  # sorted descending\n",
    "        top_item_indices_sorted = candidate_items[top_idx]\n",
    "\n",
    "        # Map indices back to gmap_ids\n",
    "        top_item_ids = [idx_to_item_id[i] for i in top_item_indices_sorted]\n",
    "\n",
    "        recs[user_id] = top_item_ids\n",
    "\n",
    "    return recs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8671ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hit_at_k = evaluate_hit_rate_at_k(\n",
    "#     modelBPR,\n",
    "#     items_per_user_train,\n",
    "#     test_items_per_user,\n",
    "#     all_items,\n",
    "#     TOP_K,\n",
    "# )\n",
    "\n",
    "# print(f\"HitRate@{TOP_K}: {hit_at_k:.4f}\" if hit_at_k is not None else \"No users with test items to evaluate.\")\n",
    "\n",
    "recommendation = get_top_k_recommendations(\n",
    "    modelBPR,\n",
    "    items_per_user_train,\n",
    "    test_items_per_user,\n",
    "    all_items,\n",
    "    TOP_K,\n",
    "    idx_to_user_id,\n",
    "    idx_to_item_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6471b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000304052561681853\n",
      "['0x54ea92b04ed31491:0xf497b5d87639810', '0x54eb2c9d759f3191:0xe3b93c57067a899a', '0x54c5b55eda22e2a9:0x1f87d215776a4838', '0x54bff8cd95e81b37:0x20ebe35b7c3faec2', '0x54b8c632c16b82f3:0xc2979d3396b38cf0', '0x54955c451f42e579:0xf61e6c7552a070a1', '0x54c0b0d955ef6c6b:0x506a85cae9ce9896', '0x54c0e23c13ef825d:0x9d110bf00de7aa33', '0x87ed7de3f0a078b3:0x6c233c3709ef1cc4', '0x54950a2eb856b805:0xe7e720b09eddfc26', '0x54c11de4265cd325:0xa3a7d53c0aae45d3', '0x54bfedda86d4a279:0x3d55c036fa2ccf71', '0x54b8a6aaaaaaaaab:0x8440cb69bc8cdd24', '0x549618d8d788f7cd:0xe1cbc1f79d7ed701', '0x54bffe899860d069:0x477233dbe7157358', '0x54cf7bb81d124cd7:0x2e2cf576e39bfd1', '0x54be1eaaa37312a3:0x6738b4912b8ca4f2', '0x54bfff0c4f6796df:0x6eae6fd7e411983', '0x5495a40b739922e1:0x1386e3022fac1181', '0x54959ad6cca60deb:0xc5a043e2f635f', '0x5495a2ae617645a7:0x9975eb0f90219342', '0x54bfff731a23c4c9:0x6309553425b0e578', '0x54c11c1a4910471f:0xc03728bb729561f5', '0x54bff8c93cfa3507:0xa6c31cf6985002d6', '0x54950c04c07d5089:0x5b2f345008809c32', '0x54950a72e8b112d1:0xd074694827faf84c', '0x54ea8c2fcc40c41d:0xd8bdbe7037c5cda8', '0x54ea8d9f62f967dd:0xb49279106da2834b', '0x5495a752d90af2bd:0xac4da428b0c0c9a7', '0x5495a0b4338cb23b:0xdf44bd5a7cbcbde4']\n"
     ]
    }
   ],
   "source": [
    "print(list(recommendation.keys())[1])\n",
    "print(recommendation[list(recommendation.keys())[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8e8421b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to  eval/bpr_one_state_recommendation_per_user.json\n",
      "Saved to  eval/users_hidden_likes.json\n"
     ]
    }
   ],
   "source": [
    "# Save the recommendations to a file so it can be used in eval.ipynb\n",
    "save_likes(\"bpr_one_state_recommendation_per_user.json\", recommendation)\n",
    "\n",
    "# Also save the hidden likes\n",
    "users_hidden_likes = defaultdict(list)\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    user_id = idx_to_user_id[row[\"user_idx\"]]\n",
    "    item_id = idx_to_item_id[row[\"item_idx\"]]\n",
    "    users_hidden_likes[user_id].append(item_id)\n",
    "\n",
    "# If you want a normal dict instead of defaultdict\n",
    "users_hidden_likes = dict(users_hidden_likes)\n",
    "\n",
    "save_likes(\"users_hidden_likes.json\", users_hidden_likes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
