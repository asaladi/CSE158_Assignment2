{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0012bb84",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import loader \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39f5d6",
   "metadata": {},
   "source": [
    "# Loader Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6052f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    # Open in text mode ('rt') with UTF-8 encoding for JSON lines\n",
    "    path = \"datasets/\" + path\n",
    "    with gzip.open(path, 'rt', encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            # Safely parse each line as JSON\n",
    "            yield json.loads(l)\n",
    "\n",
    "def load_to_dict(file_to_read):\n",
    "    data = []\n",
    "    try:\n",
    "        for item in readGz(file_to_read):\n",
    "            data.append(item)\n",
    "    except EOFError as e:\n",
    "        # Catching the specific EOFError indicating a corrupted file\n",
    "        print(f\"EOFError: Compressed file '{file_to_read}' ended prematurely. Error: {e}\")\n",
    "        print(f\"This often indicates a corrupted or incomplete gzip file. Successfully loaded {len(data)} items before the error.\")\n",
    "    except Exception as e:\n",
    "        # Catching other potential errors during decompression or JSON parsing\n",
    "        print(f\"An unexpected error occurred while reading '{file_to_read}': {e}\")\n",
    "        print(f\"Successfully loaded {len(data)} items before the error.\")\n",
    "    return data\n",
    "\n",
    "def save_likes(filename, data_dict):\n",
    "    filename = \"eval/\"+filename\n",
    "    with open(filename, \"w\") as fp:\n",
    "        json.dump(data_dict, fp, indent=4)\n",
    "    print(\"Saved to \", filename)\n",
    "\n",
    "def load_user_likes(filename):\n",
    "    \"\"\"\n",
    "    Load a user_likes JSON file back into a dict[user_id] = list of liked places.\n",
    "    \"\"\"\n",
    "    filename = \"eval/\"+filename\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Ensure values are lists, not sets or other types\n",
    "    return {user_id: list(likes) for user_id, likes in data.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bceff4e",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58453888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84cab03",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ceb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "test_reviews = load_to_dict(\"review-Oregon_10.json.gz\")\n",
    "test_metadata = load_to_dict(\"meta-Oregon.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each users' highly reviewed stores list\n",
    "# users_likes[user_id] = [stores they rated >= 4]\n",
    "users_likes = defaultdict(set)\n",
    "dupe_review_count = 0\n",
    "dupe_removed_count = 0\n",
    "\n",
    "for review in test_reviews:\n",
    "    user_id = review[\"user_id\"]\n",
    "    gmap_id = review[\"gmap_id\"]\n",
    "    rating = review[\"rating\"]\n",
    "\n",
    "    if gmap_id in users_likes[user_id]:\n",
    "        dupe_review_count += 1\n",
    "    \n",
    "    # Use the most recent review, meaning if a user re-reviewed a place and they didn't like it, update our set\n",
    "    if gmap_id in users_likes[user_id] and rating < 4:\n",
    "        users_likes[user_id].remove(gmap_id)\n",
    "        dupe_removed_count += 1\n",
    "\n",
    "    if rating >= 4 and (gmap_id not in users_likes[user_id]):\n",
    "        users_likes[user_id].add(gmap_id)\n",
    "\n",
    "print(\"Num dupes: \", dupe_review_count) \n",
    "print(\"Num dupes removed: \", dupe_removed_count) \n",
    "\n",
    "# Split off the users_likes to revealed and hidden\n",
    "users_revealed_likes = defaultdict(list)\n",
    "users_hidden_likes = defaultdict(list)\n",
    "users_total_likes = defaultdict(list)\n",
    "\n",
    "random.seed(42)\n",
    "for user_id, liked_places in users_likes.items():\n",
    "    # For now let's say 8:2 ratio for revealed vs hidden\n",
    "    # Shuffle before splitting\n",
    "\n",
    "    liked_list = list(liked_places)\n",
    "    num_likes = len(liked_list)\n",
    "    \n",
    "    random.shuffle(liked_list)\n",
    "\n",
    "    # ensures at least 1 review is hidden\n",
    "    min_hidden_count = 1\n",
    "    split_point = max(min_hidden_count, int(0.2 * num_likes))\n",
    "\n",
    "    revealed = liked_list[split_point:]\n",
    "    hidden = liked_list[:split_point]\n",
    "    \n",
    "    if len(hidden) >= min_hidden_count:\n",
    "        users_revealed_likes[user_id] = revealed\n",
    "        users_hidden_likes[user_id] = hidden\n",
    "        users_total_likes[user_id] = liked_list\n",
    "\n",
    "# Save user likes: revealed, hidden, and full\n",
    "save_likes(\"users_likes_full.json\", users_total_likes)\n",
    "save_likes(\"users_revealed_likes.json\", users_revealed_likes)\n",
    "save_likes(\"users_hidden_likes.json\", users_hidden_likes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a324955",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e7d8d",
   "metadata": {},
   "source": [
    "# Evaluation and Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42e501",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the baseline doesn't need any training, we ignore the train sets and just build it off of the test set\n",
    "# We’ll use a standard baseline for ranking latent factor model, which is by always recommending the top most popular places in the testing dataset\n",
    "# “Popular” means aggregation of features from each places’ metadata; number of reviews * average rating per store\n",
    "\n",
    "# Preprocessing the data; get the number of reviews per store in the metadata\n",
    "locations_review_count = defaultdict(int)\n",
    "locations_avg_rating = defaultdict(int)\n",
    "\n",
    "# First get the count of all the reviews for each location\n",
    "for review in test_reviews:\n",
    "    locations_review_count[review[\"gmap_id\"]] += 1\n",
    "\n",
    "# Get the average rating listed in the metadata\n",
    "for metadata in test_metadata:\n",
    "    locations_avg_rating[metadata[\"gmap_id\"]] = metadata[\"avg_rating\"]\n",
    "\n",
    "# Then multiply the two collected data and fill in the locations_popularity[gmap_id] = number of reviews * average rating\n",
    "locations_popularity = defaultdict(int)\n",
    "\n",
    "for gmap_id in locations_review_count:\n",
    "    locations_popularity[gmap_id] = locations_review_count[gmap_id] * locations_avg_rating[gmap_id]\n",
    "\n",
    "\n",
    "# Getting the resulting \"most popular\" list that can be used for the baseline\n",
    "# Turn the locations_popularity dictionary to list of tuples that we can sort\n",
    "popularity_list = [(pop, gmap_id) for gmap_id, pop in locations_popularity.items()]\n",
    "\n",
    "# Sort in reverse order so the most popular place is at the top\n",
    "popularity_list.sort(reverse=True)\n",
    "\n",
    "# And then the gmap_id only list\n",
    "popularity_list_id = [gmap_id for _, gmap_id in popularity_list]\n",
    "\n",
    "\n",
    "# Building the dictionary to feed to the evaluation function\n",
    "# recommendation[user_id] = [top k items the model recommend]\n",
    "recommendation = {}\n",
    "\n",
    "# Get each user that has reviewed\n",
    "for review in test_reviews:\n",
    "    user_id = review[\"user_id\"]\n",
    "\n",
    "    # Recommend the top number of hidden reviews for each user\n",
    "    k = 30 # 2 * len(users_hidden_likes[user_id])\n",
    "\n",
    "    if user_id not in recommendation:\n",
    "        # Filter the popularity list so that the users' revealed likes isn't included here\n",
    "        filtered_popularity_list = []\n",
    "\n",
    "        for name in popularity_list_id:\n",
    "            if name not in users_revealed_likes[user_id]:\n",
    "                filtered_popularity_list.append(name)\n",
    "\n",
    "            if len(filtered_popularity_list) == k:\n",
    "                break\n",
    "\n",
    "        recommendation[user_id] = filtered_popularity_list[:k]\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(\"baseline_recommendation_per_user.json\", \"w\") as fp:\n",
    "#     json.dump(recommendation, fp, indent=4)\n",
    "loader.save_likes(\"baseline_recommendation_per_user.json\", recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a4f1e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf62038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
