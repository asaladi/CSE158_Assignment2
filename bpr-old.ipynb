{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1349d539",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4be2d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Downloads\\CSE150B\\pa1-finding-paths-MichaelPena7\\anaconda\\envs\\cse158\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from loader import load_to_dict, load_user_likes, save_likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89538b",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "584d60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"review-Washington.json.gz\"\n",
    "\n",
    "POS_THRESHOLD = 4\n",
    "MIN_INTERACTIONS_PER_USER = 2\n",
    "MAX_USERS = 100000\n",
    "MAX_ITEMS = 100000\n",
    "\n",
    "LATENT_DIM = 20\n",
    "LEARNING_RATE = 0.01\n",
    "REG_LAMBDA = 1e-5\n",
    "NSAMPLES_PER_BATCH = 50000\n",
    "N_TRAIN_STEPS = 100\n",
    "TOP_K = 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38bac91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16541734,\n",
       "                  user_id              name           time  rating  \\\n",
       " 0  101193190259063567920       Ian Hampton  1627207184860     5.0   \n",
       " 1  103093043835388050629     Casper Steele  1626907411534     2.0   \n",
       " 2  111014066796803341223         Judy Maes  1613028426989     5.0   \n",
       " 3  112375894485126147782     Jengibre Caro  1590119768856     3.0   \n",
       " 4  111724423355988809570  Daniel Hernandez  1536710665852     5.0   \n",
       " \n",
       "                                                 text  pics  resp  \\\n",
       " 0  Drivers are helpful with directions or assista...  None  None   \n",
       " 1  Drivers say security has bathroom key. Securit...  None  None   \n",
       " 2  It's a CTRAN transit center with schedules to ...  None  None   \n",
       " 3  Positives: usually clean, convenient location,...  None  None   \n",
       " 4  Neat little Transit Center with lots of schedu...  None  None   \n",
       " \n",
       "                                  gmap_id  \n",
       " 0  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  \n",
       " 1  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  \n",
       " 2  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  \n",
       " 3  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  \n",
       " 4  0x5495ae7d3bf7d097:0xbcbc06152a3ccebc  )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = load_to_dict(DATA_FILE)\n",
    "\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "len(df), df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7b03de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11886874, 1325104, 118903)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = df[df[\"rating\"] >= POS_THRESHOLD].copy()\n",
    "\n",
    "user_counts = df_pos[\"user_id\"].value_counts()\n",
    "eligible_users = user_counts[user_counts >= MIN_INTERACTIONS_PER_USER].index\n",
    "\n",
    "df_pos = df_pos[df_pos[\"user_id\"].isin(eligible_users)].reset_index(drop=True)\n",
    "\n",
    "len(df_pos), df_pos[\"user_id\"].nunique(), df_pos[\"gmap_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fd12ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users = df_pos[\"user_id\"].unique()\n",
    "if MAX_USERS is not None:\n",
    "    unique_users = unique_users[:MAX_USERS]\n",
    "\n",
    "df_pos = df_pos[df_pos[\"user_id\"].isin(unique_users)]\n",
    "\n",
    "unique_items = df_pos[\"gmap_id\"].unique()\n",
    "if MAX_ITEMS is not None:\n",
    "    unique_items = unique_items[:MAX_ITEMS]\n",
    "\n",
    "df_pos = df_pos[df_pos[\"gmap_id\"].isin(unique_items)].reset_index(drop=True)\n",
    "\n",
    "user_id_to_idx = {u: idx for idx, u in enumerate(unique_users)}\n",
    "item_id_to_idx = {i: idx for idx, i in enumerate(unique_items)}\n",
    "\n",
    "df_pos[\"user_idx\"] = df_pos[\"user_id\"].map(user_id_to_idx)\n",
    "df_pos[\"item_idx\"] = df_pos[\"gmap_id\"].map(item_id_to_idx)\n",
    "\n",
    "num_users = len(user_id_to_idx)\n",
    "num_items = len(item_id_to_idx)\n",
    "\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "805e1c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220034, 41098)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Sort once\n",
    "if \"time\" in df_pos.columns:\n",
    "    df_pos = df_pos.sort_values([\"user_idx\", \"time\"])\n",
    "else:\n",
    "    df_pos = df_pos.sample(frac=1.0, random_state=RANDOM_SEED)\n",
    "\n",
    "user_counts = df_pos[\"user_idx\"].value_counts()\n",
    "\n",
    "# Users with at least 2 interactions\n",
    "multi_users = user_counts[user_counts >= 2].index\n",
    "\n",
    "# 3. For users with 2+ interactions: last one is test\n",
    "test_df = (\n",
    "    df_pos[df_pos[\"user_idx\"].isin(multi_users)]\n",
    "    .groupby(\"user_idx\")\n",
    "    .tail(1)\n",
    ")\n",
    "\n",
    "train_df = df_pos.drop(test_df.index)\n",
    "\n",
    "# 5. Clean up indices\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bccdcf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220034, 50000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_train = list(\n",
    "    zip(\n",
    "        train_df[\"user_idx\"].astype(int).tolist(),\n",
    "        train_df[\"item_idx\"].astype(int).tolist(),\n",
    "        train_df[\"rating\"].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "items_per_user_train = defaultdict(set)\n",
    "for u, i, r in interactions_train:\n",
    "    items_per_user_train[u].add(i)\n",
    "\n",
    "all_items = list(range(num_items))\n",
    "\n",
    "len(interactions_train), len(all_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "774c4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRbatch(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super().__init__()\n",
    "        self.lamb = lamb\n",
    "\n",
    "        # Global item bias\n",
    "        self.betaI = self.add_weight(\n",
    "            name=\"betaI\",\n",
    "            shape=(num_items,),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # User latent factors\n",
    "        self.gammaU = self.add_weight(\n",
    "            name=\"gammaU\",\n",
    "            shape=(num_users, K),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Item latent factors\n",
    "        self.gammaI = self.add_weight(\n",
    "            name=\"gammaI\",\n",
    "            shape=(num_items, K),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def score(self, sampleU, sampleI):\n",
    "        # sampleU, sampleI are index tensors\n",
    "        u = tf.cast(sampleU, tf.int32)\n",
    "        i = tf.cast(sampleI, tf.int32)\n",
    "\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "\n",
    "        x_ui = beta_i + tf.reduce_sum(gamma_u * gamma_i, axis=1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        # BPR loss: -log σ(x_ui - x_uj)\n",
    "        loss = -tf.reduce_mean(tf.math.log_sigmoid(x_ui - x_uj))\n",
    "        return loss\n",
    "\n",
    "    def reg(self):\n",
    "        return self.lamb * (\n",
    "            tf.nn.l2_loss(self.betaI)\n",
    "            + tf.nn.l2_loss(self.gammaU)\n",
    "            + tf.nn.l2_loss(self.gammaI)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "567d76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "modelBPR = BPRbatch(LATENT_DIM, REG_LAMBDA)\n",
    "\n",
    "def trainingStepBPR(model, interactions, items_per_user, items, Nsamples):\n",
    "    sampleU, sampleI, sampleJ = [], [], []\n",
    "\n",
    "    for _ in range(Nsamples):\n",
    "        u, i, r = random.choice(interactions)\n",
    "        j = random.choice(items)\n",
    "        while j in items_per_user[u]:\n",
    "            j = random.choice(items)\n",
    "\n",
    "        sampleU.append(u)\n",
    "        sampleI.append(i)\n",
    "        sampleJ.append(j)\n",
    "\n",
    "    # Convert lists → tensors\n",
    "    sampleU_tf = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "    sampleI_tf = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "    sampleJ_tf = tf.convert_to_tensor(sampleJ, dtype=tf.int32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(sampleU_tf, sampleI_tf, sampleJ_tf)\n",
    "        loss += model.reg()\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Pair gradients with variables, skipping any None grads just in case\n",
    "    grads_and_vars = [\n",
    "        (g, v) for g, v in zip(grads, model.trainable_variables) if g is not None\n",
    "    ]\n",
    "\n",
    "    if grads_and_vars:\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "    else:\n",
    "        print(\"Warning: no gradients to apply this step.\")\n",
    "\n",
    "    return float(loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5aa43ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One mini-batch objective: 0.6932\n"
     ]
    }
   ],
   "source": [
    "test_loss = trainingStepBPR(\n",
    "    modelBPR,\n",
    "    interactions_train,\n",
    "    items_per_user_train,\n",
    "    all_items,\n",
    "    Nsamples=10000  # mini testing\n",
    ")\n",
    "\n",
    "print(f\"One mini-batch objective: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0a0d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5, objective = 0.6821\n",
      "Step 10, objective = 0.6698\n",
      "Step 15, objective = 0.6582\n",
      "Step 20, objective = 0.6478\n",
      "Step 25, objective = 0.6372\n",
      "Step 30, objective = 0.6271\n",
      "Step 35, objective = 0.6190\n",
      "Step 40, objective = 0.6075\n",
      "Step 45, objective = 0.5976\n",
      "Step 50, objective = 0.5863\n",
      "Step 55, objective = 0.5735\n",
      "Step 60, objective = 0.5607\n",
      "Step 65, objective = 0.5485\n",
      "Step 70, objective = 0.5430\n",
      "Step 75, objective = 0.5339\n",
      "Step 80, objective = 0.5292\n",
      "Step 85, objective = 0.5265\n",
      "Step 90, objective = 0.5233\n",
      "Step 95, objective = 0.5193\n",
      "Step 100, objective = 0.5164\n"
     ]
    }
   ],
   "source": [
    "for step in range(N_TRAIN_STEPS):\n",
    "    obj = trainingStepBPR(\n",
    "        modelBPR,\n",
    "        interactions_train,\n",
    "        items_per_user_train,\n",
    "        all_items,\n",
    "        NSAMPLES_PER_BATCH,\n",
    "    )\n",
    "    if (step + 1) % 5 == 0:\n",
    "        print(f\"Step {step + 1}, objective = {obj:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878e65d",
   "metadata": {},
   "source": [
    "Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18f31974",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items_per_user = defaultdict(list)\n",
    "for _, row in test_df.iterrows():\n",
    "    u = int(row[\"user_idx\"])\n",
    "    i = int(row[\"item_idx\"])\n",
    "    test_items_per_user[u].append(i)\n",
    "\n",
    "def evaluate_hit_rate_at_k(model, train_items_per_user, test_items_per_user, items, k):\n",
    "    users = list(test_items_per_user.keys())\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    # Convert items to a NumPy array once\n",
    "    all_items_array = np.array(items, dtype=np.int32)\n",
    "\n",
    "    for idx, u in enumerate(users):\n",
    "        train_items = train_items_per_user[u]  # this is already a set\n",
    "\n",
    "        # Mask out training items\n",
    "        candidate_mask = ~np.isin(all_items_array, list(train_items))\n",
    "        candidate_items = all_items_array[candidate_mask]\n",
    "\n",
    "        if len(candidate_items) == 0:\n",
    "            continue\n",
    "\n",
    "        u_list = np.full(len(candidate_items), u, dtype=np.int32)\n",
    "        scores = model.score(u_list, candidate_items).numpy()\n",
    "\n",
    "        top_k_idx = np.argpartition(-scores, k - 1)[:k]\n",
    "        top_k_items = set(candidate_items[top_k_idx])\n",
    "\n",
    "        test_items = set(test_items_per_user[u])  # usually size 1\n",
    "        if top_k_items & test_items:\n",
    "            hits += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        # Optional: progress print every 1000 users so you know it's moving\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"Evaluated {idx + 1}/{len(users)} users...\")\n",
    "\n",
    "    if total == 0:\n",
    "        return None\n",
    "    return hits / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8671ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1000/41098 users...\n",
      "Evaluated 2000/41098 users...\n",
      "Evaluated 3000/41098 users...\n",
      "Evaluated 4000/41098 users...\n",
      "Evaluated 5000/41098 users...\n",
      "Evaluated 6000/41098 users...\n",
      "Evaluated 7000/41098 users...\n",
      "Evaluated 8000/41098 users...\n",
      "Evaluated 9000/41098 users...\n",
      "Evaluated 10000/41098 users...\n",
      "Evaluated 11000/41098 users...\n",
      "Evaluated 12000/41098 users...\n",
      "Evaluated 13000/41098 users...\n",
      "Evaluated 14000/41098 users...\n",
      "Evaluated 15000/41098 users...\n",
      "Evaluated 16000/41098 users...\n",
      "Evaluated 17000/41098 users...\n",
      "Evaluated 18000/41098 users...\n",
      "Evaluated 19000/41098 users...\n",
      "Evaluated 20000/41098 users...\n",
      "Evaluated 21000/41098 users...\n",
      "Evaluated 22000/41098 users...\n",
      "Evaluated 23000/41098 users...\n",
      "Evaluated 24000/41098 users...\n",
      "Evaluated 25000/41098 users...\n",
      "Evaluated 26000/41098 users...\n",
      "Evaluated 27000/41098 users...\n",
      "Evaluated 28000/41098 users...\n",
      "Evaluated 29000/41098 users...\n",
      "Evaluated 30000/41098 users...\n",
      "Evaluated 31000/41098 users...\n",
      "Evaluated 32000/41098 users...\n",
      "Evaluated 33000/41098 users...\n",
      "Evaluated 34000/41098 users...\n",
      "Evaluated 35000/41098 users...\n",
      "Evaluated 36000/41098 users...\n",
      "Evaluated 37000/41098 users...\n",
      "Evaluated 38000/41098 users...\n",
      "Evaluated 39000/41098 users...\n",
      "Evaluated 40000/41098 users...\n",
      "Evaluated 41000/41098 users...\n",
      "HitRate@10: 0.0291\n"
     ]
    }
   ],
   "source": [
    "hit_at_k = evaluate_hit_rate_at_k(\n",
    "    modelBPR,\n",
    "    items_per_user_train,\n",
    "    test_items_per_user,\n",
    "    all_items,\n",
    "    TOP_K,\n",
    ")\n",
    "\n",
    "print(f\"HitRate@{TOP_K}: {hit_at_k:.4f}\" if hit_at_k is not None else \"No users with test items to evaluate.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse158",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
